~~ Copyright 2007, by the California Institute of Technology.
~~ ALL RIGHTS RESERVED. United States Government Sponsorship acknowledged.
~~
~~ $Id: index.apt 790 2008-03-09 19:51:47Z shardman $

  ---
  Operation
  ---
  Mike Gangl
  ---
  
Operation

  This document describes how to use the Common Crawler application. The applications detailed in this document allow the user to specify two directories, remote (FTP or SFTP) or local, and crawl the files of one and write them to the other. Options and instructions are listed below.

  Each section corresponds with an available application. The following sections can be found in this document:

   * {{{#Generic_Crawler}Generic Crawler}}


{Generic Crawler}

  As mentioned above, the Generic Crawler  allows the crawling of a local or remote directory, and either write the files to another directory (local or remote) or list them for the user. Other options include custom date ranges, cleaning files of a certain age in the write directory, and defining a frequency with which to run the crawler.

* Environment Setup

  In the extracted folder, there is a command useCommon.csh.
  
+--
% source useCommon.csh 
+--
  
  This will set the environment variable COMMONC to your current directory, and will add the required *.jar files in /lib to your CLASSPATH.

* Execution

  The execution of the crawler is not dependent on I&T or OPS, so the same commands can be used for both. "cd" into the /bin directory from where you unarchived the common-crawler program set.

+--
% csh start_crawler.csh
+--

  Running the command without options is the same as running it with the -h (--help otpion):
  
+--
% csh start_crawler.csh 
Missing option: r
usage: start_crawler.pl [-c <arg>] [-E <arg>] [-f <arg>] [-h] [-home
       <arg>] [-l] [-log <arg>] [-n <arg>] [-p <arg>] [-R] [-r <arg>] [-ru <arg>]
       [-rup <arg>] [-S <arg>] [-s <arg>] [-w <arg>] [-wu <arg>] [-wup <arg>]
start_crawler
 -A,--Active                  Set the crawlers FTP mode to "active"
 -c,--Clean <arg>             The Cleaning frequency, in days, for
                              deleting files from the write directory. For example -c 5 means any file
                              that is more than 5 days old in the write directory will be deleted.
 -cmd                         Command to run when a new file is received. Pass parameters to the 
                              command using $fileName, $fileSize, $modified, $checksum, $providerURL,
                              $outputPath 
 -E,--enddate <arg>           The end date for the crawler to filter files
                              on, in yyyy-MM-dd'T'HH:mm:ss.SSS format.
 -f,--frequency <arg>         Frequency, in minutes, with which to run the
                              crawler
 -h,--help                    Print this usage information
 -home,--home <arg>           Absolute path to users home directory
 -l,--list                    List the files from the target (read)
                              location that have not yet been crawled.
 -log,--log <arg>             Path + name of logfile to output information
                              to
 -n,--name <arg>              Name for the crawler. Must be uniuque to any
                              other crawlers running on the current system
 -p,--pattern <arg>           Pattern for filtering the read directory
 -R,--Recursive               Set crawler to Recursive mode
 -r,--readdir <arg>           Properties file name
 -ru,--readuser <arg>         User's name for read directory
                              authentication
 -rup,--readuserpass <arg>    User's password for read directory
                              authentication
 -S,--startdate <arg>         The start date for the crawler to filter
                              files on, in yyyy-MM-dd'T'HH:mm:ss.SSS format.
 -save,--save-after <arg>     Number of files to process before saving to
                              the registry.
 -s,--states <arg>            State file name
 -tz, --timezone <arg>		  The timezone for which start/stop times apply.
 							  GMT or PST are examples. Local Timezone is the default.
 -w,--writedir <arg>          Message Level
 -wu,--writeuser <arg>        User's name for write directory
                              authentication
 -wup,--writeuserpass <arg>   User's password for write directory
                              authentication
+--

More information on the options is listed below

*--++
|| <<Option>> || <<Description>> |
*--++
| Active | Set the FTP Crawling mode to active, passive by default. |
*--++
| Clean <arg> | The frequency (in days) with which to clean the writedir entered below. This is done at the end of each crawl, so the window for deleting files rolls as the crawler crawls. Clean will also delete empty directories when the write directory is a local one. For example -c 5 will clean all files crawled more than 5 days ago. |
*--++
| cmd <arg>	| This is a command to invoke upon receiving or listing a new file. Reserved in the names include $fileName, $fileSize, $modified, $checksum, $providerURL, $outputPath, which can be placed into the command. For example -cmd 'mv $outputPath /path/to/new/$filename'. Single quotes must be used around the actual command.  
*--++
| enddate <arg> | enddate determines a latest file modified time that you will transport over. This requires the use of the -starttime. If we enter 2009-04-22T12:00:00, no files NEWER than april 22nd, 2009 at noon will be crawled ever. This is not a rolling window. |
*--++
| frequency <arg> | The frequency, in minutes, to wait between crawls. |
*--++
| help | List the above help message |
*--++
| home <arg> | REQUIRED specify the home directory for the user. Important to sue correct home if using public key authentication |
*--++
| list | Option is used to list files in a directory to STDOUT. |
*--++
| log <arg> | path and name of a logfile. If not specified, .cralwer.log is used in the home directory. |
*--++
| name <arg> | REQUIRED The name of the crawler. This is used to uniquely identify your crawler and create the stop file. file is created in the "home" directory specified. More information below. |
*--++
| pattern <arg>| The pattern you want to match in the readdir. If not specified, all files are crawled. Notes on pattern are below. |
*--++
| Recursive | sets the crawler to read the readdir recursively. |
*--++
| readdir <arg> | REQUIRED This is the directory you will be reading your files from. examples include file:///absolute/path/to/files, sftp://server.name.gov/path/to/file, and ftp://server.name.gov/path/to/file | |
*--++
| readuser <arg> |  The user name needed for reading from the read directory: example "guest" or "podaac" |
*--++
| readuserpass <arg> | The password for writing to the write directory. This is a conveniece option generally for FTPs with logins, but can be used by any method needing authentication. Notes on this are found below. |
*--++
| startdate <arg> | startdate determines a oldest file modified time that you will transport over. If we enter 2009-04-22T12:00:00, no files OLDER than april 22nd, 2009 at noon will be crawled ever. This is not a rolling window.|
*--++
| save <arg> | This parameter sets the save-to-state threshol|d. If set, for example, to 10, after 10 files are crawled and processed, the statefile will be updated. The default behavior was to update after an entire crawl. |
*--++
| states <arg> | REQUIRED States is a file location (which the account can write to) which houses the information from previous crawls. This ensures the same files will not be copied over. Delete the statefile or point to a new location to start crawling a directory freshly. |
*--++
| timezone <arg>| The timezone for which the start/stop times is applied. This should no longer be needed, but the default is the GMT timezone. This should be set based on the returned FTP information. |
*--++
| writedir <arg>| This is the directory you will be writing your files to. examples include file:///absolute/path/to/files, sftp://server.name.gov/path/to/file, and ftp://server.name.gov/path/to/file |
*--++
| writeuser <arg>| The user name needed for writing to the write directory: example "guest" or "podaac" |
*--++
| writeuserpass <arg>| The password for writing to the write directory. This is a conveniece option generally for FTPs with logins, but can be used by any method needing authentication. Notes on this are found below. |
*--++

[]

* Special execution notes

	*If -enddate is spcified, a startdate must also be specified.

	*If no writedir is specified, the -l option must be present. If no -l option is specified, the program will error and exit.

	*If both -l and -w are specified, the program defaults to the -l option, as this is less costly than writing the files.
	
	* If the read site requires a null password, (i.e. user and blank password) then use -rup DUMMY_PASSWORD and this will be filtered out by the crawler into a 'blank'. You probably shouldn't use 'DUMMY_PASSWORD' as a real password, in this case.

[]

{Run Commands}

* Running the crawler once, recursively (SFTP - Local)

+--
% csh start_crawler.csh -home /Users/homedir -s /Users/homedir/statefile.state -r sftp://lapinta.jpl.nasa.gov/home/data/ -n CrawlerName -ru username -w file:///Users/date/temp3 -R
+--

* Running the crawler 12 times/day (SFTP - SFTP)

+--
% csh start_crawler.csh -home /Users/homedir -s /Users/homedir/statefile.state -r sftp://lapinta.jpl.nasa.gov/home/data/ -n CrawlerName -ru username -w sftp://lapinta.jpl.nasa.gov/home/date/temp3 -wu writeUser -f 120 
+--

* Listing files remoteley (SFTP)

+--
% csh start_crawler.csh -home /Users/homedir -s /Users/homedir/statefile.state -r sftp://lapinta.jpl.nasa.gov/home/data/ -n CrawlerName -l -ru username -rup userpass -S 2009-04-23T00:00:00
+--

* Cleaning files that are five days old days (SFTP - Local)

+--
% csh start_crawler.csh -home /Users/homedir -s /Users/homedir/statefile.state -r sftp://lapinta.jpl.nasa.gov/home/data/ -n CrawlerName -ru username -w file:///Users/date/temp3 -C 5
+--

[]

{Important Information}

* Stopping the crawler

	If you're running the crawler continuously (-f) you'll want to know a clean way of shutting it down. upon startup, you must give the "home" and "name" options. These together create a file in the home directory called '.crawler.crawnername' where crawlername is the name specified. As long as this file exists, the crawler will still run. Delete this file to stop the crawler after its current run, or before it runs again if it is sleeping.

+--
	
[User-mac:~] user% ls -al .crawler.*
-rw-r--r--  1 user  staff      18 Apr 23 10:40 .crawler.crawlername
-rw-r--r--  1 user  staff  164857 Apr 23 10:40 .crawler.log
[User-mac:~] user% rm .crawler.crawlername
[User-mac:~] user% 
	
+--


*patterns

	The pattern expression is only slightly different than normal regular Expressions. Due to the commands being interpreted by both java and the cshell, and ease of use, the '*' is replaced by the # for specifying the pattern.
	
	#.txt matches all files ending in .txt
	#txt# matches all files with txt in the name
	FDF# matches all files starting with FDF
	
	The pattern ability is fairly routine, and doesn't support all the bells and whistles of a true regular expression. 

*authentication
	
	Authentication can be done two different ways. Both involve using the -ru and -wu options to specify a user name. Passwords can be givein via the command line using the -rup and -wup options, along with the passwords. This is good for anonnomous ftp sites, but works for any login site. The other method is detailed here:
	
+--
Here is the instruction for setting up ssh public key so you can
avoid putting your password in the .m2/settings.xml file used
by Maven.

login to any podaac linux dev system, lapinta for example.
do
ssh-keygen -P '' -t dsa
cd ~/.ssh
 
If you don't already have an "authorized_keys" file simply
cat id_dsa.pub > authorized_keys

If you already have an "authorized_keys" file append
cat id_dsa.pub >> authorized_keys

This setup will allow you to jump from one PODAAC linux DEV machine
to another using ssh without being prompted for password.

If you want to have similiar setup from your Mac to podaac linux dev

on your mac run
ssh-keygen -P '' -t dsa
cd ~/.ssh
now copy your id_dsa.pub to lapinta

on lapinta
cd ~/.ssh
append your Mac's id_dsa.pub to authorized_keys file.

The instruction here is "openssh" specific which works on all
of PODAAC linux machine and your desktop macs.  The current
PODAAC non-linux machine (ie seastar) uses a different "ssh" 
software and additional steps are needed to setup the public key. 
+--

[]

* Running commands (-cmd)

   As stated above, the crawler has the ability to invoke a command upon receipt of a new file. The basic format is the following:

+--
-cmd 'command <options>'
+--

   Where command can be the path to any command you'd like to run on the system the crawler is running on.

   Special options include $fileName, $fileSize, $modified, $checksum, $providerURL, and $outputPath.

*--++
|| <<Option>> || <<Description>> |
*--++
|$fileName	|	The name of the file received, without path information|
*--++
|$fileSize	|	The size, in bytes, of file received|
*--++
|$modified	|	The last modified time, in Long format of the file received|
*--++
|$checksum	|	Checksum of the file received. This is only available on files written to the local file system|
*--++
|$providerURL|	The URL of the file retreived|
*--++
|*outputPath|	The path the file was written to, only availble when the -l (list) option is not used|
*--++
 
