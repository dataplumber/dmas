~~ Copyright 2008, by the California Institute of Technology.
~~ ALL RIGHTS RESERVED. United States Government Sponsorship acknowledged.
~~
~~ $Id: index.apt 14484 2015-11-25 00:18:25Z gangl $

  ---
  Operation
  ---
  Mike Gangl
  ---
  
Operation

  This document describes how to use the Distribute software. Each section corresponds with an available application. The following sections can be found in this document:

   * {{{Setup for IWS}Setup for IWS}}

   * {{{createCollection.sh}createCollection.sh}}

   * {{{echoExport.sh}echoExport.sh}}
   
   * {{{EMSReport.sh}EMSReport.sh}}
   
   * {{{Datacast.sh}Datacast.sh}}   

   * {{{gcmdExport.sh}gcmdExport.sh}}

   * {{{rollingStoreExport.sh}rollingStoreExport.sh}}

{Setup for IWS}

  Setting up the distribute tools for using the Inventory Web Service is straight forward. In the distribute.config file, you have the following applicable options:

+--
gov.nasa.podaac.inventory.factory=gov.nasa.podaac.distribute.common.wsm.Query
#gov.nasa.podaac.inventory.factory=gov.nasa.podaac.distribute.common.direct.Query
inventory.ws.url=https://lanina
inventory.ws.port=http://9192
inventory.ws.user=
inventory.ws.password=
+--

New in 4.4 are some other configurables, such as the ECHO token and CMR servers:
+--

podaac.cmr.host=https://cmr.uat.earthdata.nasa.gov/ingest/
podaac.token.host=https://api-test.echo.nasa.gov/echo-rest/tokens
podaac.echo.auth.file=/usr/local/daac/distribute/bin/echo.tokens
+--


  The web services version is set by default, but simply comment the "wsm" line and uncomment "direct" and you'll use the legacy hibernate system. 
  
  The URL, User and Password fields must be set accordingly for using the web service.  
  
  Before you export to CMR, you'll need to set up your username and password authfile. This script saves your URS username and password in an encrypted file so the program can read it later.

+--
sh echoAuth.sh
Generating distribute password tokens.
Please enter your URS username: 
mike.gangl
Please enter your URS password: 

encrypted string: pYRMWprv6lrh3n2NjmicNg==
encrypted string: wcW+lkAVgK20DhgKNiX4gA==
+--

  This creates an 'echo.tokens' file in the directory you run it in. This file must be pointed to by the 'podaac.echo.auth.file' configurable above.

{echoExport.sh}

  echoExport.sh is a utility which will create granule information for export to ECHO.

+--
sh echoExport.sh 
usage: ECHOExport [-D] [-d <datasetId>] [-dl <dId1,dId2,dId3,...>] [-fb]
       [-g] [-l <arg>] [-o] [-t] [-v <path>] [-x]
 -D,--dataset                              Export dataset file; Used with
                                           -d
 -d,--did <datasetId>                      Dataset Persistent ID
 -dl,--dataset-list <dId1,dId2,dId3,...>   List of dataset persistentIds
 -fb,--fb                                  Forces the use of bounding box
                                           for spatial element in export.
 -g,--granule                              Export granule files of the
                                           given collectionId; Used with
                                           'c';
 -l,--limit <arg>                          limit the exporting of granules
                                           instead of exporting them all.
 -o,--output-xml                           Enables output XML files to the
                                           data directory.
 -t,--test-only                            Does not update
                                           echo_submission_times after
                                           granules are sent to CMR.
 -x,--no-transfer                          Do not submit granules,
                                           collections to CMR.
+--


  Besides c/C being replaced with d/D in the 4.4 version, 4 new options exist to help the test and usability of the tool.
  
  -l lets you limit the number of granules exported at any time. If no -l option is given, ALL granules are exported.
  
  -t this flag does not update the echo_submission_time for a granule, so you can reprocess it again.
  
  -x prvents the granules and collections from being transfered to CMR.
  
  -o outputs the granules and collections to the 'data' directory. No output is done by default unless an error occurs. All erroneous granules and collections are output to that directory.

  For example, the following command will create the granule lists for collection 11 into a set of collection files:

+--
% sh echoExport.sh -d PODAAC-GHN16-2PE01 -g -D
+--

  And the log file's output will read:

+--
2009-06-12 14:02:58,289 INFO  ECHOExport:99 - processGranules:11
2009-06-12 14:03:03,296 INFO  ECHOGranuleFile:149 - Process collectionId=11 datasetId=28
2009-06-12 14:03:28,432 INFO  ECHOGranuleFile:185 - granuleSize=581
2009-06-12 14:04:34,522 INFO  ECHOGranuleFile:196 - Processing Granule [99]: 20050201-NAR16_SST-EUR-L2P-sst1nar_noaa16_20050201_asc-v01.nc
2009-06-12 14:04:39,852 INFO  ECHOGranuleFile:196 - Processing Granule [100]: 20050201-NAR16_SST-EUR-L2P-sst1nar_noaa16_20050201_desc-v01.nc
2009-06-12 14:04:43,892 INFO  ECHOGranuleFile:196 - Processing Granule [171]: 20050202-NAR16_SST-EUR-L2P-sst1nar_noaa16_20050202_asc-v01.nc
2009-06-12 14:04:47,717 INFO  ECHOGranuleFile:196 - Processing Granule [172]: 20050202-NAR16_SST-EUR-L2P-sst1nar_noaa16_20050202_desc-v01.nc
....
+--

  The output files are specified in the config/distribute.config file, where one can also set the number of granules to break each file into. For example, if this is set to 100, there will only be 100 granule files per collection.xml file. There will be mutliple files if there are over 100 granules.
  
  The -fb option will force the program to use the granuleReal entries to create a bounding-box, spatial envelope when exporting individual granules. 
  
{EMSReport.sh}

  EMSReport.sh is a utility that will create EMS report files for ingest and archive.

+--
  sh EMSReport.sh -help
  usage: EMSReport [-archive] [-d <date>] [-ed <endDate>] [-h] [-ingest] [-pid] [-o
       <output>]
  -archive,--archive                Create the archhive report
  -d,--report-date <date>           The date on which to run the report. If
                                   not specified, yesterday's date is used. Date is in the format MM/DD/YYYY.
  -ed,--report-end-date <endDate>   The end date with which to run the
                                   reporting tool. The report-date option is required if this option is used.
                                   Date is in the format MM/DD/YYYY. A report will not be generated for the
                                   the date entered here (non-inclusive).
  -gsr,--granule-status-report <gsr>   The directory containing granule
                                      status reports
  -h,--help                         Display help and usage information
  -ingest,--ingest                  Create the ingest report
  -legacyId,--use-legacyId          Flag to use the legacy dataset id
                                   (numeric) when creating reports.
  -o,--output-path <output>         The path where the output file(s) will
                                   be written.
  -pid,--productId		    Flag to use Product IDs instead of Dataset IDs
+--

  -archive or -ingest, and an output path (-o) are required to run the tool.

  Ingest and archive reports should be generated for each day by specifying the -archive and -ingest flags (you can run both flags at the same time). 
  
  By default, the program runs for "yesterday" in calendar days. You can specify a date on which to run the report by using the -d option. To run the report across several days, specify a beginning date and an end date (-ed). Files will be split based on which day the file was archived or ingested.
  
  The files need an output path to be written to, specified by the -o option. An example would be something like -o /store/EMS and all files will be written to the EMS directory. Files are automatically versioned if a previous report currently exists. Newer files will have a ".rev" suffix followed by a number (the higher the number, the more recent the version.

  By default, the program will use the permanent Dataset ID for output. This is new as of release 3.1.0. If You want to sue the numeica dataset_id, you must specify the -legacyId option. The -pid option will continue to use the product_id as defined in the collection tables.
  
  The following command will run archive and ingest reports, using the product ID instead of dataset ID, and store the files in the /store/EMS directory, and create 2 files:
+--
% sh EMSReport.sh -archive -ingest -pid -o /store/EMS
+--

  The following command will run the ingest report for the first 3 weeks of february (and will create 21 files):

+--
% sh EMSReport.sh -ingest -o /store/EMS -d 02/01/2010 -ed 02/21/2010 -legacyId
+--


{Datacast.sh}

  Datacast.sh is a utility that will create datacast granule items which are simply key=value pairs in plain text.

+--
  sh Datacast.sh 
  usage: Datacast [-d <date>] [-dataset <dataset>] [-ed <endDate>] [-h] [-o
       <output>]
  -d,--report-date <date>           The date on which to run the report. If
                                   not specified, yesterday's date is used. Date is in the format MM/DD/YYYY.
  -dataset,--dataset <dataset>      The dataset on which to run datacasting
  -ed,--report-end-date <endDate>   The end date with which to run the
                                   reporting tool. The report-date option is required if this option is used.
                                   Date is in the format MM/DD/YYYY. A report will not be generated for the
                                   the date entered here (non-inclusive).
  -h,--help                         Display help and usage information
  -o,--output-path <output>         The path where the output files will be written.
  -shortname,--use-shortname        Flag to use the legacy shortname to
                                   find and create products

+--

  Note: As of release 3.1.0, the tool now defaults to using the dataset permanent ID. To continue running the tool with dataset short names, specify the -shortname option. The previous note about usage here used to read: This tool does not use dataset ids, but rather dataset shortnames. If the shortname is not found the user will be alreted to this. 

  The output of this tool will be a granule file of key=value pairs to be ingested by the datacast IngestItems script. It is the first step in the datacast process.

  Only a dataset name and output path are required to run the tool. Like the EMS tool above, a start and end date can be spcified to run the reports over a single or series of past days.

  The report defaults to "yesterday" as a default time range for archival.  

  The output path, while it can be any location the user has permission to write to, should be stored in a data directory under the datacast directories. Each dataset to be datacast will need  "granules," "items-xml," and "queue" directories for the datacast service to work correctly. The output of this tool should be placed in the "granules" subdirectory.

  The following commands will run the datacast tool on JPL-L2P-MODIS_A data:

+--
% sh Datacast.sh -d JPL-L2P-MODIS_A -shortname -o /usr/local/daac/datacasts/feedExample/granules
+--

  The following commands will run the datacast tool on JPL-L2P-MODIS_A data for a past day:

+--
% sh Datacast.sh -d 04/22/2009 -dataset JPL-L2P-MODIS_A -shortname -o /usr/local/daac/datacasts/feedExample/granules
+--


{gcmdExport.sh}

  gcmdExport.sh is a utility that will create DIF XML files for submission to the Global Change Master Directory (GCMD).
  
+--
  sh gcmdExport.sh --help
  usage: GCMDExport [-dataset <ds1,ds2,ds3,...>] [-h] [-shortname] [-v]
  -dataset,--dataset <ds1,ds2,ds3,...>   Dataset persistent ID or short
                                         name
  -h,--help                              Display help and usage information
  -shortname,--use-shortname             Flag to use the legacy short name
                                         to find dataset and generate DIF XML file
  -v,--validate                          Validate resulting XML file(s)
  
+--

  By default, the values specified for the -dataset option is assumed to be dataset persistent ID. Use -shortname option to specify dataset shortname rather than dataset persistent ID.
  
  Use -v option to validate the generated XML file(s) against the DIF XML schema version 9.8.2.
  
  DIF XML file for each dataset will be generated and stored in the directory that is specified in config/distribute.config file as gcmd.data.location.
  
  In config/distribute.config, the user can also specify default values that will be used in the resulting DIF XML file.  For example, by specifying gcmd.iso.topic.category=Oceans the following element will be appended to the XML file: <ISO_Topic_Category>Oceans</ISO_Topic_Category>.  Note that all contacts exported to the XML file(s) will have the same address as specified in config/distribute.config.
  
  The following command will generate DIF XML files for dataset PODAAC-AQUAR-0SORM and PODAAC-GH17G-2PN01 and will check that the resulting XML files conform to DIF XML schema:

+--
% sh gcmdExport.sh -dataset PODAAC-GHAMS-2PE01,PODAAC-EMTGE-WAVA1 -v
+--

  The log file's output will read:

+--
2011-03-23 10:35:23,381 INFO  GCMDExport:69 - Process dataset: PODAAC-GHAMS-2PE01
2011-03-23 10:35:25,252 INFO  GCMDDatasetFile:171 - Processing Dataset [5]: PODAAC-GHAMS-2PE01
2011-03-23 10:35:25,984 INFO  GCMDExport:58 - /usr/local/daac/distribute/gcmd/PODAAC-GHAMS-2PE01.xml conforms to DIF schema
2011-03-23 10:35:25,984 INFO  GCMDExport:85 - Process dataset PODAAC-GHAMS-2PE01 completed.
2011-03-23 10:35:25,984 INFO  GCMDExport:69 - Process dataset: PODAAC-EMTGE-WAVA1
2011-03-23 10:35:26,091 INFO  GCMDDatasetFile:171 - Processing Dataset [124]: PODAAC-EMTGE-WAVA1
2011-03-23 10:35:26,875 INFO  GCMDExport:58 - /usr/local/daac/distribute/gcmd/PODAAC-EMTGE-WAVA1.xml conforms to DIF schema
2011-03-23 10:35:26,875 INFO  GCMDExport:85 - Process dataset PODAAC-EMTGE-WAVA1 completed.
....
+--


{rollingStoreExport.sh}

  rollingStoreExport.sh is a utility that will create a text file containing a listing of granules with their archive time, ftp path, checksum.

+--
  sh rollingStoreExport.sh --help
  usage: RollingStoreExport [-d <day>] [-dl <persistentId1,persistentId2,persistentId3,...>] [-f <file>] [-h]
  -d,--daysAgo <number of days>                                       Archived n days ago
  -dl,--datasetList <persistentId1,persistentId2,persistentId3,...>   List of persistent IDs
  -f,--file <file>                                                    Full path to index file
  -h,--help                                                           Display help and usage
                                                                      information
  
+--

  If --daysAgo is not specified, the default is 30 days.

  In config/distribute.config, user can specify a comma separated list of persistent IDs as value of noaa.rolling.store.dataset.list. Note this value will be overridden if the command line option --datasetList is provided.

  The following command will generate a text file at /Users/nchung/index.txt for dataset PODAAC-GHVRS-2PN02, PODAAC-GHOST-4FK02, and PODAAC-GHGDM-4FD02:

+--
% sh rollingStoreExport.sh --datasetList PODAAC-GHVRS-2PN02,PODAAC-GHOST-4FK02,PODAAC-GHGDM-4FD02 --daysAgo 30 --file /usr/local/daac/distribute/noaa/index.txt
+--

  The log file's output will read:

+--
2013-10-23 15:06:52,621 INFO  RollingStoreExport:142 - Running RollingStoreExport with the following parameters:
2013-10-23 15:06:52,622 INFO  RollingStoreExport:143 - Dataset List: [PODAAC-GHVRS-2PN02, PODAAC-GHOST-4FK02, PODAAC-GHGDM-4FD02]
2013-10-23 15:06:52,625 INFO  RollingStoreExport:144 - Number of days: 30
2013-10-23 15:06:52,625 INFO  RollingStoreExport:145 - Index file: /usr/local/daac/distribute/noaa/index.txt
2013-10-23 15:06:52,625 INFO  RollingStoreExport:146 - distribute.solr.url (from distribute.config file): http://localhost:8983/solr.war/granule
2013-10-23 15:06:52,626 INFO  RollingStoreExport:148 - distribute.solr.items.per.page (from distribute.config file): 1000
2013-10-23 15:06:52,626 INFO  RollingStoreExport:205 - Search for granules where archive time <= 1379974012626
2013-10-23 15:07:02,025 INFO  RollingStoreExport:234 - Total granules found for dataset PODAAC-GHVRS-2PN02 : 126566
2013-10-23 15:07:36,635 INFO  RollingStoreExport:295 - Finished exporting granules for dataset: PODAAC-GHVRS-2PN02
2013-10-23 15:07:36,646 INFO  RollingStoreExport:234 - Total granules found for dataset PODAAC-GHOST-4FK02 : 151
2013-10-23 15:07:36,648 INFO  RollingStoreExport:295 - Finished exporting granules for dataset: PODAAC-GHOST-4FK02
2013-10-23 15:07:36,659 INFO  RollingStoreExport:234 - Total granules found for dataset PODAAC-GHGDM-4FD02 : 146
2013-10-23 15:07:36,661 INFO  RollingStoreExport:295 - Finished exporting granules for dataset: PODAAC-GHGDM-4FD02
2013-10-23 15:07:36,661 INFO  RollingStoreExport:297 - Index file created at: /usr/local/daac/distribute/noaa/index.txt
2013-10-23 15:07:36,675 INFO  RollingStoreExport:318 - Total execution time: 44060 milliseconds
+--

